{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Computing | AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Cloud Computing?\n",
    "Cloud computing: the practice of using a network of remote servers hosted on the Internet to store, manage, and process data, rather than a local server or a personal computer.\n",
    "\n",
    "The arrival of cloud computing completely changed the way we deploy our technology, providing powerful access to instant and scalable computing power to enterprises, startups, and developers alike. Whether you need servers to host a web application, reliable storage for your data, or machines to train machine learning models, it's easy to see the advantage of relying on the cloud rather than utilizing your personal computer or local servers.\n",
    "\n",
    "For one, you no longer have to invest in lots of hardware upfront. No need to worry about whether you are paying for more than you'll need or what to do if you need to scale a lot more later on. Cloud computing makes this as easy and clicking a few buttons to scale your resources up or down.\n",
    "\n",
    "It's significantly faster provisioning the resources you need through the cloud versus the time it would take to gather and build up the hardware you'd need to provide the same support. This allows you and your team, or company, to develop and experiment at a much faster rate.\n",
    "\n",
    "Lastly, you can provide efficient access to your applications around the world by spreading your deployments to multiple regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Web Services (AWS)\n",
    "\n",
    "Amazon Web Services is one of the largest providers in the cloud computing industry, with over 140 services in compute, storage, databases, networking, developer tools, security, and more. In this lesson, we'll learn about a few essential tools and services in AWS and practice using them. These services can be accessed in three different ways: the AWS Management Console, the Command Line Interface (CLI), or Software Development Kits (SDKs), which can be used in combination.\n",
    "\n",
    "We'll start with the AWS Management Console, which is the web user interface. The AWS CLI is a useful way to control and automate your services with code, and SDKs allow you to easily integrate services with your applications through APIs built around specific languages and platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Data Wareshouse on AWS\n",
    "Pre-requisites \n",
    "- Relational database design and SQL \n",
    "- Programming in Python \n",
    "- Dimensional modelling and creating OLAP cubes \n",
    "- Basic ETL in Python \n",
    "- AWS services: IAM, VPC, 53, EC2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ETL PROCESS**\n",
    "\n",
    "**Step 1:** Data Sources: Different types, skill sets, upgrades locations, etc. (high heterogeneity)   \n",
    "**Step 2:** ETL: Many processes - a 'grid\" of machines with different schedules and pipeline complexities \n",
    "**Step 3:** DWH: More resources need to be added as data increases. We have different workloads; some need one machine and some need many (scalability & elasticity)   \n",
    "**Step 4:** Business Intelligence Apps & Visualizations: Also need a hybrid deployment of tools for interaction, reporting, visualizations, etc.  \n",
    "\n",
    "#### Choices for Implementing a Data Warehouse\n",
    "\n",
    "**On-premise:** \n",
    "1. Heterogeneity, scalability, elasticity of the tools, technologies, and processes \n",
    "2. Need for diverse IT staff skills & multiple locations.\n",
    "3. Cost of ownership \n",
    "\n",
    "**On-cloud:** \n",
    "- Lower barrier to entry\n",
    "- May add as you need - its ok to change your opinion \n",
    "- Scalability & elasticity out of the box \n",
    "- Operational cost might be high and heterogeneity/complexity won't disappear, but...   \n",
    "    - **On-Cloud we have two options**\n",
    "        - **Cloud-Managed:** Re-use of expertise; way less IT Staff for security, upgrades, etc. and way less OpEx Deal with complexity with techniques like: \n",
    "          Infrastructure as code\" \n",
    "          - **(Amazon RDS, Amazon DynamoDB Amazon S3)** \n",
    "        - **Self-Managed:** Always \"catch-all\" option if needed \n",
    "            - **(EC2 + Postgresql, EC2 + Cassandra, EC2 + Unix FS)**\n",
    "            \n",
    "<img src=\"images/image13.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Redshift Technology  (Inside postgres with modifications)\n",
    "\n",
    "- Most relational databases execute multiple queries in parallel if they have access to many cores/servers \n",
    "- However, every query is always executed an a single CPU of a single machine \n",
    "- Acceptable for OLTP, mostly updates and few rows retrieval \n",
    "\n",
    "<img src=\"images/image14.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "- Massively Parallel Processing (MPP) databases parallelize the execution of one query on multiples CPUs/machines \n",
    "- How? A table is partitioned and partitions are processed in parallel \n",
    "- Amazon Redshift is a cloud-managed, column- oriented, MPe database \n",
    "- Other examples include Teradata Aster, Oracle ExaData and Azure SQL \n",
    "\n",
    "<img src=\"images/image15.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Architecture: The Cluster\n",
    "\n",
    "Redshift Cluster:\n",
    "- 1 Leader node\n",
    "    - LeaderNode\n",
    "    - Coordinates compute nodes \n",
    "    - Handles external communication \n",
    "    - Optimizes query execution \n",
    "\n",
    "- 1+ Compute node\n",
    "    - Each with own CPU, memory, and disk (determined by the node type)\n",
    "    - Scale up: get more powerful nodes\n",
    "    - Scale out: get more nodes \n",
    "\n",
    "    - **Node Slices:** \n",
    "        - Each compute node is logically divided into a number of slices \n",
    "        - A cluster with n slices, can process n partitions of a tables simultaneously \n",
    "  \n",
    "1. **The total number of nodes in a Redshift cluster is equal to:** The number of AWS EC2 instances used in the cluster \n",
    "2. **Each slice in a Redshift cluster is:** At least 1 CPU with dedicated storage and memory for the slice \n",
    "3. **If we have a Redshift cluster with 4 nodes, each containing 8 slices, i.e. the cluster collectively offers 32 slices. What is the maximum number of partitions per table?** 32 Partitions\n",
    "\n",
    "<img src=\"images/image16.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"images/image17.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"images/image18.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL to SQL ETL\n",
    "<img src=\"images/image20.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"images/image21.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift & ETL in Context\n",
    "<img src=\"images/image22.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "**Ques:** Why do you think we might need to copy data already stored in S3 to another S3 staging bucket during the ETL process?  \n",
    "**Ans:**  Because it would be transformed before insertion into the DWH "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting at Scale: **Use COPY** \n",
    "- To transfer data from an 53 staging area to redshift use the **COPY** command \n",
    "- Inserting data row by using **INSERT** will be very slow \n",
    "- If the file is large: o It is better to break it up to **multiple files** \n",
    "- Ingest in **Parallel** \n",
    "    - Either using a **common prefix** \n",
    "    - Ora **manifest file**.\n",
    "- Other considerations: \n",
    "    - Better to ingest from the same AWS region \n",
    "    - Better to compress the all the csv files \n",
    "        - One can also specify the delimiter to be used \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift ETL Examples: \n",
    "\n",
    "#### Common Prefix\n",
    "<img src=\"images/image23.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "#### Manifest File\n",
    "<img src=\"images/image24.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "### Redshift ETL Automatic Compression Optimization \n",
    "- The optimal compression strategy for each column type is different \n",
    "- Redshift gives the user control over the compression of each column \n",
    "- The COPY command makes automatic best-effort compression decisions for each column \n",
    "\n",
    "### ETL from Other Sources \n",
    "- It is also possible to ingest directly using ssh from EC2 machines \n",
    "- Other than that: \n",
    "    - 53 needs to be used as a staging area \n",
    "    - Usually, an EC2 ETL worker needs to run the ingestion jobs orchestrated by a dataflow product like Airflow, Luigi, Nifi, StreamSet or AWS Data Pipeline \n",
    "\n",
    "### ETL Out of Redshift \n",
    "- Redshift is accessible, like any relational database, as a JDBC/ODBC source \n",
    "    - Naturally used by BI apps \n",
    "- However, we may need to extract data out of Redshift to pre-aggregated OLAP cubes\n",
    "\n",
    "<img src=\"images/image25.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Redshift Cluster: More Details \n",
    "- The cluster created by the Quick Launcher is a fully-functional one, but we need more functionality... \n",
    "- Security: \n",
    "    - The cluster is accessible only from the virtual private cloud \n",
    "    - We need to access it from our jupyter workspace \n",
    "- Access to S3: \n",
    "    - The cluster needs to access an s3 bucket \n",
    "\n",
    "### Configuring Redshift for S3 and external access \n",
    "- Naturally, we can accomplish our goal by going through lots of sereenshetsMelees or \"elk-lc-al:A-:4H\" instructions \n",
    "- That said, we take this as an opportunity to introduce an important technique for modern data engineers, namely: Infrastructure-as-Code (laC) \n",
    "- An advantage of being in the cloud is the ability to create infrastructure, i.e. machines, users, roles, folders and processes using code, \n",
    "- laC lets you automate, maintain, deploy, replicate and share complex infrastructures as easily as you maintain code (undreamt-of in an on-premise deployment). e.g. \"Creating a machine is as easy as opening a file\" \n",
    "- To be honest laC is border-line dataEng/devOps \n",
    "\n",
    "### We have a number of options to achieve laC on AWS \n",
    "- aws-cli scripts \n",
    "- AWS sdk \n",
    "- Amazon Cloud formation (Template)\n",
    "\n",
    "### Which of the following are advantages of Infrastructure-as-Code over creating infrastructure by clicking-around? \n",
    "- Sharing: One can share all the steps with others easily \n",
    "- Reproducibility: One can be sure that no steps are forgotten environment \n",
    "- Multiple deployments: One can create a test environment identical to the production the code \n",
    "- Maintainability: If a change is needed, one can keep track of the changes by comparing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boto3 \n",
    "It is a Python SDK for programmatically accessing AWS. It enables developers to create, configure, and manage AWS services. You can find the documentation for Boto3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Table Design\n",
    "\n",
    "- When a table is partitioned up into many pieces and distributed across slices in different machines, this is done blindly \n",
    "- If one has an idea about the frequent access pattern of a table, one can choose a more clever strategy \n",
    "- The 2 possible strategies are: \n",
    "    - Distribution Style \n",
    "        - EVEN distribution  \n",
    "        - ALL distribution \n",
    "        - AUTO distribution \n",
    "        - KEY distribution \n",
    "    - Sorting key \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Style: EVEN\n",
    "<img src=\"images/image26.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<img src=\"images/image27.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<img src=\"images/image28.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Style: All\n",
    "<img src=\"images/image29.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<img src=\"images/image30.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Syle: Auto\n",
    "- Leave decision to Redshift \n",
    "- \"Small enough\" tables are distributed with an ALL strategy \n",
    "- Large tables are distributed with EVEN strategy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Syle: Key\n",
    "<img src=\"images/image31.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "- Rows having similar values are placed in the same slice \n",
    "- This can lead to a skewed distribution if some values of the dist key are more frequent than others \n",
    "- However, very useful when a dimension table is too big to be distributed with ALL strategy. In that case, we distribute both the a fact table and the dimension table using the same dist key. \n",
    "- If two tables are distributed on the joining keys, redshift collocates the rows from both tables on the same slices \n",
    "<img src=\"images/image32.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Key \n",
    "- One can define its columns as sort key \n",
    "- Upon loading, rows are sorted before distribution to slices \n",
    "- Minimizes the query time since each node already has contiguous ranges of rows based on the sorting key \n",
    "- Useful for colurnns that are used frequently in sorting like the date dimension and its corresponding foreign key in the fact table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
